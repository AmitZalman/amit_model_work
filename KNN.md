# KNN – K Nearest Neighbors  
סיכום שאלות ותשובות

---

## שאלה 1:  
**האם האלגוריתם KNN פותר בעיית קלסיפיקציה, רגרסיה או גם וגם?**

**תשובה:**  
גם קלסיפיקציה וגם רגרסיה.

---

## שאלה 2:  
**מה המשמעות של השם KNN?**

**תשובה:**  
K Nearest Neighbors –  
K מייצג את מספר השכנים הקרובים ביותר שבהם המודל משתמש לצורך קבלת החלטה.

---

## שאלה 3:  
**מהו הרעיון המרכזי שעליו מבוסס האלגוריתם KNN?**

**תשובה:**  
האלגוריתם KNN מבוסס על כך שדוגמה חדשה מסווגת או מקבלת ערך  
לפי K הדוגמאות הקרובות אליה ביותר במרחב הפיצ’רים.

---

## שאלה 4:  
**מהו התפקיד של הפרמטר K באלגוריתם?**

**תשובה:**  
K קובע כמה שכנים קרובים ישפיעו על התחזית של המודל.

---

## שאלה 5:  
**האם K נחשב Hyperparameter? הסבר/י.**

**תשובה:**  
כן.  
K הוא Hyperparameter משום שהוא נקבע מראש על־ידי המשתמש  
ואינו נלמד מהנתונים במהלך הריצה.

---

## שאלה 6:  
**כיצד בחירת ערך קטן של K משפיעה על המודל?**

**תשובה:**  
ערך קטן של K עלול לגרום ל־Overfitting,  
משום שהמודל מושפע מאוד מרעש ונקודות בודדות.

---

## שאלה 7:  
**כיצד בחירת ערך גדול של K משפיעה על המודל?**

**תשובה:**  
ערך גדול של K עלול לגרום ל־Underfitting,  
משום שהמודל מבצע ממוצע על פני יותר מדי נקודות  
ומאבד פרטים חשובים.

---

## שאלה 8:  
**מהו גרף Elbow וכיצד משתמשים בו לבחירת K אידיאלי?**

**תשובה:**  
גרף Elbow מציג את איכות המודל כפונקציה של K.

- בציר X: ערכי K  
- בציר Y: מדד שגיאה (MSE ברגרסיה, Accuracy בקלסיפיקציה)  

נקודת ה־Elbow (ה"מרפק") היא המקום שבו שיפור הביצועים מתחיל להאט,  
וזו לרוב הבחירה האופטימלית ל־K.

דוגמה נוספת לשימוש ב־Elbow:  
ב־K-Means Clustering לבחירת מספר הקלאסטרים.

---

## שאלה 9:  
**מה ההבדל בין KNN לקלסיפיקציה לבין KNN לרגרסיה?**

**תשובה:**  
- בקלסיפיקציה – ההחלטה מתקבלת לפי הצבעת רוב של השכנים  
- ברגרסיה – התחזית מתקבלת באמצעות ממוצע (או ממוצע משוקלל) של ערכי ה־Y של השכנים

---

## שאלה 10:  
**כיצד מתקבלת התחזית ב־KNN לקלסיפיקציה?**

**תשובה:**  
המודל מחשב מרחקים (לרוב מרחק אוקלידי),  
בוחר את K השכנים הקרובים ביותר  
ומסווג לפי המחלקה הדומיננטית ביניהם.

---

## שאלה 11:  
**כיצד מתקבלת התחזית ב־KNN לרגרסיה?**

**תשובה:**  
התחזית מתקבלת על־ידי חישוב ממוצע (או ממוצע משוקלל לפי מרחק)  
של ערכי ה־Y של K השכנים הקרובים ביותר.

---

## שאלה 12:  
**איזו מדידת מרחק נפוצה משמשת ב־KNN?**

**תשובה:**  
מרחק אוקלידי (Euclidean Distance),  
אם כי ניתן להשתמש גם במרחקים אחרים כמו Manhattan או Cosine.

---

## שאלה 13:  
**כיצד מספר הפיצ'רים משפיע על ביצועי KNN?**

**תשובה:**  
ככל שמספר הפיצ'רים גדל, המרחב הופך לדליל יותר  
(תופעת Curse of Dimensionality),  
מה שפוגע בדיוק המודל ומאט את החישוב.

---

## שאלה 14:  
**האם קיים שלב אימון (Training) מובהק באלגוריתם KNN?**

**תשובה:**  
לא.  
KNN הוא אלגוריתם מסוג Lazy Learning –  
הוא שומר את הנתונים, וכל החישוב מתבצע בזמן החיזוי.

---

## שאלה 15:  
**כיצד Python פותר את חישוב KNN – נוסחה סגורה או חישוב ישיר?**

**תשובה:**  
באמצעות חישוב ישיר בזמן החיזוי  
(חישוב מרחקים לכל הדוגמאות).

---

## שאלה 16:  
**מהם היתרונות המרכזיים של KNN?**

**תשובה:**  
- אלגוריתם פשוט ואינטואיטיבי  
- אין שלב אימון מורכב  
- מתאים במיוחד לדאטה קטן  

---

## שאלה 17:  
**מהם החסרונות המרכזיים של KNN?**

**תשובה:**  
- חישוב איטי בזמן חיזוי  
- רגיש מאוד לבחירת K  
- רגיש לסקיילינג של הפיצ'רים  
- ביצועים יורדים במימדים גבוהים  

---
